#!/usr/bin/env python3

import os
import json
import gc
import pickle
import sys
import getopt

from tqdm import tqdm, trange
import pandas as pd
import numpy as np

from sklearn import linear_model
from joblib import dump, load
import torch
from transformers import BertTokenizer, BertConfig
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from keras.preprocessing.sequence import pad_sequences

from transformers import BertForTokenClassification, AdamW

from sklearn.metrics import precision_recall_fscore_support

def main(argv):

    # list of intents
    intents = []
    # list of entities
    entities = []

    try:
        opts, args = getopt.getopt(argv, "ima:", ["input_file=", "models_path=", "assets_path="])
        input_file = opts[0][1]
        models_path = opts[1][1]
        assets_path = opts[2][1]

        intents = open(assets_path+"/intents.txt").read().split("\n")[:-1]
        entities = open(assets_path+"/entities.txt").read().split("\n")[:-1]


        # data_test is a list containing every sentence in which we will perform intent and entity
        # recognition
        with open(input_file) as f:
            data_test = f.read().split("\n")[:-1]

        # we associate an ID for each entity, and an ID for each intent, and two dictionaries for the mapping
        dict_intents = {intents[i]: i for i in range(len(intents))}
        dict_entities = {entities[i]: i for i in range(len(entities))}

        reverse_dict_intents = {v: k for k, v in dict_intents.items()}
        reverse_dict_entities = {v: k for k, v in dict_entities.items()}

        # we load the BERT tokenizer
        tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)

        # input for the inference model
        tokenized_data_test = []

        for sentence in data_test:
            tokenized_sentence = []
            for word in sentence.split():
                tokenized_word = tokenizer.tokenize(word)
                tokenized_sentence.append(tokenized_word)

            tokenized_data_test.append([item for sublist in tokenized_sentence for item in sublist])

        # hyperparameters

        with open(assets_path+"/hyperparameters.json") as f:
            hyperparameters = json.load(f)

        MAX_LEN = hyperparameters["MAX_LEN"]
        bs = hyperparameters["batch_size"]

        # here, we convert every subword in sentences to IDs, using BERT tokenizer
        input_ids_test = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_data_test], maxlen=MAX_LEN, dtype="long", value=0.0, truncating="post", padding="post")


        # for sentence where we use padding, we add mask tokens to ignore it during the training

        attention_masks_test = [[float(i != 0.0) for i in ii] for ii in input_ids_test]

        #===================================
        #     1. Intention Detection 
        #===================================

        print("Inference for Intent Classification")

        # we load the model
        model_intents = load(models_path+"/model_intent.joblib")

        intents_predictions = model_intents.predict(input_ids_test)

        #===================================
        #     2 : Entity Detection
        #===================================

        input_ids_test = torch.tensor(input_ids_test)
        attention_masks_test = torch.tensor(attention_masks_test)

        # we convert the data to load it by batch to the model

        test_data = TensorDataset(input_ids_test, attention_masks_test)
        test_dataloader = DataLoader(test_data, batch_size=bs)

        # Entity_Recognition is the model we use for entity detection

        class Entity_Recognition(torch.nn.Module):

            def __init__(self):
                super(Entity_Recognition, self).__init__()
                self.bert = BertForTokenClassification.from_pretrained("bert-base-cased",
                                        num_labels = len(entities),
                                        output_attentions = False,
                                        output_hidden_states = False)

            def forward(self, x, attention_masks):
                x = self.bert(x, attention_mask=attention_masks)
                x = x[0]
                x = x.view(-1, len(entities), MAX_LEN)
                return x

        model_entities = Entity_Recognition()
        
        # we load the parameters of the model

        checkpoint = torch.load(models_path+"/model_entities.pt", map_location=lambda storage, loc: storage)
        from collections import OrderedDict
        new_checkpoint = OrderedDict()
        for k, v in checkpoint.items():
            new_k = k[7:]
            new_checkpoint[new_k] = v

        model_entities.load_state_dict(checkpoint)

        # ========================================
        #               Inference
        # ========================================

        predictions = []
        for nb_batch, batch in enumerate(test_dataloader):
            batch = tuple(t for t in batch)
            b_input_ids, b_input_mask = batch

            with torch.no_grad():
                # Forward pass, calculate logit predictions
                # This will return the logits rather than the loss because we have not provided labels
                outputs = model_entities(b_input_ids, attention_masks=b_input_mask)
            # Move logits and labels to CPU
            logits = outputs.detach().cpu().numpy()
            predictions = [list(p) for p in np.argmax(logits, axis=1)]

            for i, prediction in enumerate(predictions):
                print(data_test[nb_batch*bs + i])
                print("intent : ", reverse_dict_intents[intents_predictions[nb_batch*bs + i]]) 
                #print("entities : ", [reverse_dict_entities[p] for p in prediction if reverse_dict_entities[p]!="PAD"])
                # these are the sentence and prediction when we deal with words, and not subwords
                new_tokens, new_prediction = [], []
                for token, label_idx in zip(tokenizer.convert_ids_to_tokens(input_ids_test[nb_batch*bs + i]), prediction):
                    if token.startswith("##"):
                        new_tokens[-1] = new_tokens[-1] + token[2:]
                    elif token == "[PAD]":
                        continue
                    else:
                        new_prediction.append(reverse_dict_entities[label_idx])
                        new_tokens.append(token)

                for token, label in zip(new_tokens, new_prediction):
                    print("{}\t{}".format(label, token))
                print()

    except getopt.GetoptError:
        print("error, required evaluate --i <input_file> --m <models_path> --a <assets_path>")

if __name__ == "__main__":
    main(sys.argv[1:])
